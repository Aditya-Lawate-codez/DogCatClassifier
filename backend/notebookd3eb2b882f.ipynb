{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4981434,"sourceType":"datasetVersion","datasetId":2889098}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-16T05:32:35.857461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:34:30.219379Z","iopub.execute_input":"2024-03-16T05:34:30.219798Z","iopub.status.idle":"2024-03-16T05:34:41.046195Z","shell.execute_reply.started":"2024-03-16T05:34:30.219720Z","shell.execute_reply":"2024-03-16T05:34:41.045021Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-16 05:34:32.103166: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-16 05:34:32.103278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-16 05:34:32.253692: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model = keras.Sequential(\n    [\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(128, (3, 3), activation='relu'),\n    layers.MaxPooling2D((2, 2)),\n    layers.Flatten(),\n    layers.Dense(512, activation='relu'),\n    layers.Dense(1, activation='sigmoid')\n    ]\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:35:42.945334Z","iopub.execute_input":"2024-03-16T05:35:42.945908Z","iopub.status.idle":"2024-03-16T05:35:43.220657Z","shell.execute_reply.started":"2024-03-16T05:35:42.945849Z","shell.execute_reply":"2024-03-16T05:35:43.219787Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:35:53.550885Z","iopub.execute_input":"2024-03-16T05:35:53.551188Z","iopub.status.idle":"2024-03-16T05:35:53.564825Z","shell.execute_reply.started":"2024-03-16T05:35:53.551164Z","shell.execute_reply":"2024-03-16T05:35:53.563746Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:36:08.510286Z","iopub.execute_input":"2024-03-16T05:36:08.511289Z","iopub.status.idle":"2024-03-16T05:36:08.516008Z","shell.execute_reply.started":"2024-03-16T05:36:08.511262Z","shell.execute_reply":"2024-03-16T05:36:08.514955Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# /kaggle/input/30k-cats-and-dogs-150x150-greyscale/Animal Images","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:36:24.010112Z","iopub.execute_input":"2024-03-16T05:36:24.010430Z","iopub.status.idle":"2024-03-16T05:36:24.014177Z","shell.execute_reply.started":"2024-03-16T05:36:24.010406Z","shell.execute_reply":"2024-03-16T05:36:24.013446Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n        '/kaggle/input/30k-cats-and-dogs-150x150-greyscale/Animal Images',\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:36:36.942761Z","iopub.execute_input":"2024-03-16T05:36:36.943104Z","iopub.status.idle":"2024-03-16T05:36:49.488826Z","shell.execute_reply.started":"2024-03-16T05:36:36.943079Z","shell.execute_reply":"2024-03-16T05:36:49.487988Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Found 30061 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\nvalidation_generator = validation_datagen.flow_from_directory(\n        '/kaggle/input/30k-cats-and-dogs-150x150-greyscale/Animal Images',\n        target_size=(150, 150),\n        batch_size=20,\n        class_mode='binary')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:37:01.463196Z","iopub.execute_input":"2024-03-16T05:37:01.463535Z","iopub.status.idle":"2024-03-16T05:37:08.151141Z","shell.execute_reply.started":"2024-03-16T05:37:01.463510Z","shell.execute_reply":"2024-03-16T05:37:08.150206Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Found 30061 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=100,\n      epochs=30,\n      validation_data=validation_generator,\n      validation_steps=50)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:37:10.352900Z","iopub.execute_input":"2024-03-16T05:37:10.353253Z","iopub.status.idle":"2024-03-16T05:55:12.720617Z","shell.execute_reply.started":"2024-03-16T05:37:10.353228Z","shell.execute_reply":"2024-03-16T05:55:12.719913Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 373ms/step - accuracy: 0.5214 - loss: 0.7177 - val_accuracy: 0.5050 - val_loss: 0.6927\nEpoch 2/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 368ms/step - accuracy: 0.4994 - loss: 0.6942 - val_accuracy: 0.5190 - val_loss: 0.6887\nEpoch 3/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 368ms/step - accuracy: 0.5692 - loss: 0.6809 - val_accuracy: 0.6600 - val_loss: 0.6263\nEpoch 4/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 365ms/step - accuracy: 0.6490 - loss: 0.6361 - val_accuracy: 0.6090 - val_loss: 0.6939\nEpoch 5/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 372ms/step - accuracy: 0.6697 - loss: 0.6177 - val_accuracy: 0.6820 - val_loss: 0.6108\nEpoch 6/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 368ms/step - accuracy: 0.6544 - loss: 0.6207 - val_accuracy: 0.6770 - val_loss: 0.6058\nEpoch 7/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 370ms/step - accuracy: 0.6801 - loss: 0.6028 - val_accuracy: 0.7030 - val_loss: 0.5785\nEpoch 8/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 370ms/step - accuracy: 0.6890 - loss: 0.5964 - val_accuracy: 0.7010 - val_loss: 0.5769\nEpoch 9/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 372ms/step - accuracy: 0.6949 - loss: 0.5825 - val_accuracy: 0.7170 - val_loss: 0.5548\nEpoch 10/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 371ms/step - accuracy: 0.7042 - loss: 0.5691 - val_accuracy: 0.7280 - val_loss: 0.5497\nEpoch 11/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 370ms/step - accuracy: 0.7440 - loss: 0.5362 - val_accuracy: 0.7240 - val_loss: 0.5626\nEpoch 12/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 371ms/step - accuracy: 0.7166 - loss: 0.5480 - val_accuracy: 0.7730 - val_loss: 0.4896\nEpoch 13/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 370ms/step - accuracy: 0.7625 - loss: 0.5218 - val_accuracy: 0.7610 - val_loss: 0.5128\nEpoch 14/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 369ms/step - accuracy: 0.7570 - loss: 0.5202 - val_accuracy: 0.6990 - val_loss: 0.5435\nEpoch 15/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 371ms/step - accuracy: 0.7529 - loss: 0.4995 - val_accuracy: 0.7910 - val_loss: 0.4608\nEpoch 16/30\n\u001b[1m  4/100\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 319ms/step - accuracy: 0.8604 - loss: 0.3214","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 60ms/step - accuracy: 0.8504 - loss: 0.3088 - val_accuracy: 0.7320 - val_loss: 0.5405\nEpoch 17/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 370ms/step - accuracy: 0.7746 - loss: 0.4666 - val_accuracy: 0.7720 - val_loss: 0.4648\nEpoch 18/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 370ms/step - accuracy: 0.7675 - loss: 0.4800 - val_accuracy: 0.7680 - val_loss: 0.4784\nEpoch 19/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 373ms/step - accuracy: 0.7843 - loss: 0.4641 - val_accuracy: 0.7740 - val_loss: 0.4701\nEpoch 20/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 371ms/step - accuracy: 0.7775 - loss: 0.4575 - val_accuracy: 0.8080 - val_loss: 0.4278\nEpoch 21/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 376ms/step - accuracy: 0.7706 - loss: 0.4761 - val_accuracy: 0.8130 - val_loss: 0.4296\nEpoch 22/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 375ms/step - accuracy: 0.7847 - loss: 0.4558 - val_accuracy: 0.7910 - val_loss: 0.4616\nEpoch 23/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 369ms/step - accuracy: 0.8024 - loss: 0.4295 - val_accuracy: 0.7480 - val_loss: 0.5168\nEpoch 24/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 367ms/step - accuracy: 0.7705 - loss: 0.4627 - val_accuracy: 0.7370 - val_loss: 0.5613\nEpoch 25/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 372ms/step - accuracy: 0.7885 - loss: 0.4310 - val_accuracy: 0.8370 - val_loss: 0.3776\nEpoch 26/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 371ms/step - accuracy: 0.7899 - loss: 0.4381 - val_accuracy: 0.8330 - val_loss: 0.3751\nEpoch 27/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 372ms/step - accuracy: 0.8007 - loss: 0.4127 - val_accuracy: 0.8060 - val_loss: 0.4248\nEpoch 28/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 371ms/step - accuracy: 0.8033 - loss: 0.4319 - val_accuracy: 0.8070 - val_loss: 0.4160\nEpoch 29/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 372ms/step - accuracy: 0.8148 - loss: 0.3953 - val_accuracy: 0.8250 - val_loss: 0.3783\nEpoch 30/30\n\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 376ms/step - accuracy: 0.8131 - loss: 0.3986 - val_accuracy: 0.8440 - val_loss: 0.3831\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.preprocessing import image\n\n# Load the image to predict\nimg_path = '/kaggle/input/30k-cats-and-dogs-150x150-greyscale/Animal Images/dogs/2013-04-27_443495431719958334.jpg'\nimg = image.load_img(img_path, target_size=(150, 150))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx /= 255\n\n# Make the prediction\nprediction = model.predict(x)\nif prediction < 0.5:\n    print(\"The image is a cat.\")\nelse:\n    print(\"The image is a dog.\")","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:57:26.719094Z","iopub.execute_input":"2024-03-16T05:57:26.720141Z","iopub.status.idle":"2024-03-16T05:57:26.944410Z","shell.execute_reply.started":"2024-03-16T05:57:26.720099Z","shell.execute_reply":"2024-03-16T05:57:26.942804Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\nThe image is a dog.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model\nmodel.save('dogs_cat_model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-16T05:57:51.297030Z","iopub.execute_input":"2024-03-16T05:57:51.297378Z","iopub.status.idle":"2024-03-16T05:57:51.354125Z","shell.execute_reply.started":"2024-03-16T05:57:51.297340Z","shell.execute_reply":"2024-03-16T05:57:51.353267Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}